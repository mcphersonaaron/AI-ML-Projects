{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a1a408",
   "metadata": {
    "id": "97a1a408"
   },
   "source": [
    "# Example of Data Management and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee251446",
   "metadata": {
    "id": "ee251446"
   },
   "source": [
    "In this notebook, we give an example of data management and pre-processing in the context of preparing data for machine learning models. Properly handling and transforming the raw data are essential steps that significantly impact the performance and reliability of models. We will explore various techniques using popular Python libraries, such as pandas and scikit-learn, to address common challenges in real-world datasets.\n",
    "\n",
    "Let's proceed with a practical example to understand the fundamental steps involved in data management and pre-processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a641941",
   "metadata": {
    "id": "6a641941"
   },
   "source": [
    "## Example pre-processing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631333f1",
   "metadata": {
    "id": "631333f1",
    "outputId": "5ae0b464-e5d3-408b-a0b2-a07fd7615a5c"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Sample data\n",
    "data = pd.DataFrame({\n",
    "    'Age': [25, 30, None, 35, 28],\n",
    "    'Income': [50000, 60000, 75000, None, 55000],\n",
    "    'Gender': ['M', 'F', 'M', 'F', 'M'],\n",
    "    'Loan_Status': ['Approved', 'Rejected', 'Approved', 'Approved', 'Rejected']\n",
    "})\n",
    "\n",
    "print(\"Original Data: \")\n",
    "print(data)\n",
    "\n",
    "#Handling missing values with mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[['Age', 'Income']] = imputer.fit_transform(data[['Age', 'Income']])\n",
    "\n",
    "print(\"\\nMissing Data replaced with mean: \")\n",
    "print(data)\n",
    "\n",
    "#Encoding categorical variables (Gender and Loan_Status)\n",
    "le = LabelEncoder()\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n",
    "\n",
    "print(\"\\n1-hot encoding categorical data: \")\n",
    "print(data)\n",
    "\n",
    "#Scaling numerical features (Age and Income) using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[['Age', 'Income']] = scaler.fit_transform(data[['Age', 'Income']])\n",
    "\n",
    "#Display the preprocessed and cleansed data\n",
    "print(\"\\nScaling numnerical features using StandardScaler: \")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b86fc0",
   "metadata": {
    "id": "43b86fc0"
   },
   "source": [
    "## Explanation of Example Code:\n",
    "\n",
    "**Import Libraries:**\n",
    "\n",
    "- Import necessary libraries, including pandas for data manipulation and scikit-learn for data preprocessing.\n",
    "\n",
    "**Sample Data:**\n",
    "\n",
    "- Create a sample dataset with columns for 'Age,' 'Income,' 'Gender,' and 'Loan_Status.' Introduce missing values and use categorical variables intentionally.\n",
    "\n",
    "**Handling Missing Values:**\n",
    "\n",
    "- Use SimpleImputer to handle missing values in the 'Age' column by imputing the missing values with the mean of the non-missing values.\n",
    "\n",
    "**Encoding Categorical Variables:**\n",
    "\n",
    "- Use LabelEncoder to encode categorical variables 'Gender' and 'Loan_Status' into numerical labels.\n",
    "\n",
    "**Scaling Numerical Features:**\n",
    "\n",
    "- Use StandardScaler to scale numerical features 'Age' and 'Income' to standardize them, making their values comparable.\n",
    "\n",
    "**Display Data:**\n",
    "\n",
    "- Display the preprocessed and cleansed data at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857c523",
   "metadata": {
    "id": "e857c523"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8bbd6d0",
   "metadata": {
    "id": "b8bbd6d0"
   },
   "source": [
    "# Using the Lending Club Dataset as an Example\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "In the following cells, we dive into the world of data management and pre-processing for financial services using a dataset from LendingClub. The goal is to apply essential data cleaning and transformation techniques to prepare the data for further analysis and modeling.\n",
    "\n",
    "## Tasks:\n",
    "\n",
    "1. **Data Loading:**\n",
    "   - Import the LendingClub Loan Data dataset, limiting the import to three numeric variables and three character variables.\n",
    "   - This step is spelled out in more detail below.  The Lending Club Loan Data dataset will be used in other projects, so it is best practices to load the dataset into your Jupyter Notebook directory.\n",
    "  \n",
    "2. **Data Exploration:**\n",
    "   - Conduct an initial exploration of the dataset, examining summary statistics and understanding the distribution of key variables.\n",
    "\n",
    "3. **Handling Missing Values:**\n",
    "   - Identify and handle missing values for numeric variables using an appropriate strategy (e.g., imputation).\n",
    "\n",
    "4. **Encoding Categorical Variables:**\n",
    "   - Utilize encoding techniques (e.g., one-hot encoding) for handling categorical variables.\n",
    "\n",
    "5. **Scaling Numerical Features:**\n",
    "   - Implement scaling on numeric features to standardize their values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95844e5",
   "metadata": {
    "id": "e95844e5"
   },
   "source": [
    "## LendingClub Dataset Setup\n",
    "\n",
    "### Overview:\n",
    "The LendingClub dataset will be utilized for various projects. To ensure best practices and seamless access, it's recommended to save the dataset CSV file in the same directory as your Jupyter notebook. Follow the steps below to download, extract, and load the dataset into your working directory.\n",
    "\n",
    "### Step-by-Step Guidance:\n",
    "\n",
    "1. **Download the LendingClub Dataset:**\n",
    "   - Visit the [LendingClub Dataset on Kaggle](https://www.kaggle.com/datasets/wordsforthewise/lending-club/).\n",
    "   - Click on the \"Download\" button to obtain the dataset in ZIP format.\n",
    "\n",
    "2. **Extract the Dataset:**\n",
    "   - Locate the downloaded ZIP file (e.g., `loan.zip`).\n",
    "   - Extract the contents to reveal the CSV file (`accepted_2007_to_2018q4.csv`).\n",
    "\n",
    "3. **Move the CSV to Your Notebook Directory:**\n",
    "   - Move the extracted CSV file to the directory where your Jupyter notebook resides.\n",
    "   - Alternatively, you can specify the full path to the CSV file in your notebook.\n",
    "\n",
    "4. **Load the Dataset in Your Notebook:**\n",
    "   - Use the following code to read the CSV file in your Jupyter notebook:\n",
    "     ```python\n",
    "     import pandas as pd\n",
    "\n",
    "     # Assuming the CSV file is in the same directory as your notebook\n",
    "     data = pd.read_csv('accepted_2007_to_2018q4.csv', low_memory=False)\n",
    "     ```\n",
    "\n",
    "By following these steps, you'll have the LendingClub dataset readily available for analysis in your Jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13988cac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "13988cac",
    "outputId": "04a446dd-3524-424d-add4-600f30d98f4b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# To deal with file size limitations, we only load the three numeric\n",
    "# columns and three character columns specified\n",
    "cols = ['funded_amnt', 'installment', 'annual_inc','loan_status', 'verification_status', 'home_ownership']\n",
    "data = pd.read_csv('accepted_2007_to_2018q4.csv', low_memory=False, usecols=cols)\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599067e2-c2a1-40a4-949f-cda604a8ea54",
   "metadata": {},
   "source": [
    "Now we start examining the data by generating some summary statistics. First we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039720e-6d9d-4652-b1f8-fad082efadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273985c-d4b9-4e84-92fb-bd75b9f391e0",
   "metadata": {},
   "source": [
    "Now we generate the summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1a1f9-5a2b-4dc7-b4a1-3f607e3ffeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the characteristics of the dataset\n",
    "print(data.info())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbe62f-7bf4-4382-9c1e-4f522e0b3137",
   "metadata": {},
   "source": [
    "We check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf39ef-6960-40be-9456-ddcc5064fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b4f8e-422f-406c-b284-b26f5f81b4ae",
   "metadata": {},
   "source": [
    "To fill the missing values for the numeric data, we use a simple imputer.  However, first we need to ascertain the distribution of the data in order to decide whether to use the mean or median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f325f5c-9e5c-4d75-ab53-218c7074536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print histograms of each independent variable to see the distribution\n",
    "data.hist(figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b60bb9-1420-4c4b-9ef5-8d0f9361352c",
   "metadata": {},
   "source": [
    "Since the distributions are skewed to the left, we decide to use the median rather than the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff3da9-a80a-4638-98b9-abb7d54d822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values with median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data[['funded_amnt', 'installment', 'annual_inc']] = imputer.fit_transform(data[['funded_amnt', 'installment', 'annual_inc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece09b5-3746-47f7-a79f-60d749592f00",
   "metadata": {},
   "source": [
    "For the character columns, there is no way to know what value should be inserted, so we will drop the corresponding rows, making sure to make a copy of the original dataframe so the original is not affected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a30b7f-1dea-488a-af7a-0b272e5b4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific columns to check\n",
    "columns_to_check = ['home_ownership', 'verification_status', 'loan_status']\n",
    "\n",
    "#Drop rows where any of the specified columns have missing values\n",
    "data_cleaned = data.dropna(subset=columns_to_check).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc07323-3e03-4d4d-8381-23e5bf322251",
   "metadata": {},
   "source": [
    "Now we check to make sure there are no more missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739757e-e4f3-4c96-b4c2-7376a1f55b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ffc24-a5fd-45e6-8e4c-eaa762eeb954",
   "metadata": {},
   "source": [
    "We convert the categorical variables into dummy variables using OneHotEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d78fa-217c-4e9b-897d-a35736efd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical variables (home_ownership, verification_status, and loan_status)\n",
    "# Initialize the OneHotEncoder\n",
    "# sparse=False returns a numpy array instead of a sparse matrix\n",
    "# handle_unknown='ignore' prevents errors when the encoder encounters categories not seen during fit\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "data_encoded = pd.DataFrame(encoder.fit_transform(data_cleaned[['home_ownership', 'verification_status', 'loan_status']]))\n",
    "# drop columns that have been encoded\n",
    "data_cleaned.drop(['home_ownership', 'verification_status', 'loan_status'], axis=1, inplace=True)\n",
    "# add the encoded columns back in\n",
    "data_combined = pd.concat([data_cleaned, data_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37290bb5-7ce3-4e0a-bf2d-f5f7c5f8e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see that the resulting dataframe looks right\n",
    "print(data_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cae49-439c-466e-a715-c172ac956fb0",
   "metadata": {},
   "source": [
    "Finally, we scale the numeric features so their values are standardized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d5e95-42c1-4e53-ad15-2c834f279065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling numerical features (funded_amnt, installment, and annual_inc) using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_combined[['funded_amnt', 'installment', 'annual_inc']] = scaler.fit_transform(data_combined[['funded_amnt', 'installment', 'annual_inc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205a683-44ee-42e1-9245-92fba96f1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see that the resulting dataframe looks right\n",
    "print(data_combined.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
