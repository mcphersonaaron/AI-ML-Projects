{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e974b5c4",
   "metadata": {
    "id": "e974b5c4"
   },
   "source": [
    "### Predictive Modeling - probablity of default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9319425",
   "metadata": {
    "id": "e9319425"
   },
   "source": [
    "- This code represents a typical model pipeline\n",
    "- The model pipeline steps are:\n",
    "    - Read in necessary libraries\n",
    "    - Pull the data from a webpage\n",
    "    - Split the data into train and test datasets\n",
    "    - Create a Random Forest Classifier\n",
    "    - Train the model on the train dataset\n",
    "    - Use the model to predict the test dataset\n",
    "    - Create model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d7a20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b0d7a20",
    "outputId": "2d8d3c85-96d0-4c24-b13d-b69651e19f0d"
   },
   "outputs": [],
   "source": [
    "#Import necessary libaries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "#Load the dataset\n",
    "url = 'https://github.com/Safa1615/Dataset--loan/blob/main/bank-loan.csv?raw=true'\n",
    "data = pd.read_csv(url, nrows=700)\n",
    "\n",
    "# Save to Excel\n",
    "data.to_excel('dataset.xlsx', index=False)\n",
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory, 'dataset.xlsx')\n",
    "print(f\"The file is saved at: {file_path}\")\n",
    "\n",
    "#Split the data into features (independent variables) and the target variable (default or not)\n",
    "X = data.drop('default', axis=1)\n",
    "y = data['default']\n",
    "\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Initialize a classification model (in this case, a Random Forest classifier)\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Make prediction on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Accuracy: {accuracy: .2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c82936",
   "metadata": {
    "id": "26c82936"
   },
   "source": [
    "### The provided code is a basic implementation of a Random Forest Classifier for predicting loan default. Here's a breakdown:\n",
    "\n",
    "### Data Loading:\n",
    "\n",
    "- The dataset is loaded from a GitHub repository using <em>pd.read_csv().\n",
    "\n",
    "### Data Splitting:\n",
    "\n",
    "- The data is split into features (X) and the target variable (y), which is whether a loan defaults or not.\n",
    "- Further, the dataset is split into training and testing sets using <em>train_test_split().\n",
    "\n",
    "### Model Initialization and Training:\n",
    "\n",
    "- A Random Forest classifier is initialized with 100 trees <em>(n_estimators=100) for ensemble learning.\n",
    "- The classifier is trained on the training data using <em>fit().\n",
    "\n",
    "### Prediction:\n",
    "\n",
    "- Predictions are made on the test data using <em>predict().\n",
    "\n",
    "### Model Evaluation:\n",
    "\n",
    "- Accuracy, confusion matrix, and classification report are computed using <em>accuracy_score(), confusion_matrix(), and classification_report()<em>.\n",
    "\n",
    "### Results Printing:\n",
    "\n",
    "- The results, including accuracy, confusion matrix, and classification report, are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46131cc7",
   "metadata": {
    "id": "46131cc7",
    "outputId": "2964e747-3131-4cdc-885e-b5796f45274a"
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3222e42",
   "metadata": {
    "id": "e3222e42"
   },
   "source": [
    "# Credit Risk Prediction with XGBoost\n",
    "\n",
    "## Objective:\n",
    "\n",
    "- Build an XGBoost classifier to predict credit default based on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede24190",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ede24190",
    "outputId": "3b6590a3-e0f9-4029-ee51-647d139d6d2c"
   },
   "outputs": [],
   "source": [
    "#Import necessary libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier as XGBoostClassifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt  #added to allow printing of plots\n",
    "import seaborn as sns\n",
    "\n",
    "#Load the dataset\n",
    "url = 'https://github.com/Safa1615/Dataset--loan/blob/main/bank-loan.csv?raw=true'\n",
    "data = pd.read_csv(url, nrows=700)\n",
    "\n",
    "# Save to Excel\n",
    "data.to_excel('dataset.xlsx', index=False)\n",
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory, 'dataset.xlsx')\n",
    "print(f\"The file is saved at: {file_path}\")\n",
    "\n",
    "# Print the characteristics of the dataset\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Print histograms of each independent variable to see the distribution\n",
    "data.hist(figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "# Create a box plot to visualize the distribution of 'income'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y='income', data=data)\n",
    "plt.title(\"Box Plot of Income\")\n",
    "plt.ylabel(\"Income\")\n",
    "plt.show()\n",
    "\n",
    "#Split the data into features (independent variables) and the target variable (default or not)\n",
    "X = data.drop('default', axis=1)  #create a new table X, and drop the dependent variable from the table, because the model has to predict it.\n",
    "y = data['default']               #add the dependent variable to a new table y of predictions, against which the model will be trained.\n",
    "\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  #Set aside 20% of the records for use as a test set\n",
    "\n",
    "#Initialize a classification model (in this case, a XGBoost classifier)\n",
    "classifier = XGBoostClassifier(n_estimators=100,      #Build up to 100 trees\n",
    "                               learning_rate=0.1,     #Amount by which each tree's predictions are scaled to the running prediction\n",
    "                               max_depth=6,           #Maximum number of levels of each tree - helps reduce overfitting\n",
    "                               eval_metric=\"logloss\", #Metric to use to evaluate how well each tree fits the data\n",
    "                               random_state=42)       #Seed value to make sure that each run produces the same results\n",
    "\n",
    "#Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Make prediction on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Accuracy: {accuracy: .2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sbpUDIelpz3w",
   "metadata": {
    "id": "sbpUDIelpz3w"
   },
   "source": [
    "From the data.info command, we can see that there are no missing values in any of the columns, so we will not need to use imputation.\n",
    "\n",
    "However, the accuracy and f1-score are the same as for the random forest model, so we are not getting any improvement.\n",
    "\n",
    "Looking at the histograms, we see skewing towards the left, which suggests that we may need to create new features using a logarithm transformation.  Also, we see that there is an outlier in the income column, which could be distorting the model.  The chart below shows the results for income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df070c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "81df070c",
    "outputId": "444fe65b-80c4-4cdf-dcf5-d902561d1334"
   },
   "outputs": [],
   "source": [
    "data = data[ data['income'] != 446 ]                # drop the row with the max income, from the describe table above.\n",
    "\n",
    "data['income_log'] = np.log1p(data['income'])       # take the logarthim of income\n",
    "data.hist(\"income_log\", figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmk2GDt9uV-o",
   "metadata": {
    "id": "kmk2GDt9uV-o"
   },
   "source": [
    "Now we rerun the XGBoost classifier to see if we can get a more epic result. That is, a higher accuracy or F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8VdSRTg6uom9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VdSRTg6uom9",
    "outputId": "e4d15d95-65f9-4a93-e44b-0035472daeaf"
   },
   "outputs": [],
   "source": [
    "#Split the data into features (independent variables) and the target variable (default or not)\n",
    "X = data.drop('default', axis=1)  #create a new table X, and drop the dependent variable from the table, because the model has to predict it.\n",
    "X = X.drop('income', axis=1)      #drop income because we added a new feature, 'income_log', above.\n",
    "y = data['default']               #add the dependent variable to a new table y of predictions, against which the model will be trained.\n",
    "\n",
    "print(X.info())\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  #Set aside 20% of the records for use as a test set\n",
    "\n",
    "#Initialize a classification model (in this case, a XGBoost classifier)\n",
    "classifier = XGBoostClassifier(n_estimators=100,      #Build up to 100 trees\n",
    "                               learning_rate=0.1,     #Amount by which each tree's predictions are scaled to the running prediction\n",
    "                               max_depth=6,           #Maximum number of levels of each tree - helps reduce overfitting\n",
    "                               eval_metric=\"logloss\", #Metric to use to evaluate how well each tree fits the data\n",
    "                               random_state=42)       #Seed value to make sure that each run produces the same results\n",
    "\n",
    "#Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Make prediction on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Accuracy: {accuracy: .2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TWdpmlDHvg3E",
   "metadata": {
    "id": "TWdpmlDHvg3E"
   },
   "source": [
    "Using income_log instead of income, and dropping the outlier, improved the accuracy from 0.78 to 0.81, and the f1 score from 0.86 to 0.88 for 0, and from 0.47 to 0.56 for 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b8072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "254b8072",
    "outputId": "28869b31-ffae-45db-949f-67237172c978"
   },
   "outputs": [],
   "source": [
    "#Import necessary libaries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Split the data into features (independent variables) and the target variable (default or not)\n",
    "X = data.drop('default', axis=1)  #create a new table X, and drop the dependent variable from the table, because the model has to predict it.\n",
    "X = X.drop('income', axis=1)      #drop income because we added a new feature, 'income_log', above.\n",
    "y = data['default']               #add the dependent variable to a new table y of predictions, against which the model will be trained.\n",
    "\n",
    "print(X.info())\n",
    "\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  #Set aside 20% of the records for use as a test set\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],  # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
    "    'max_depth': [3, 4, 5, 6]  # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBoostClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1',\n",
    "                           cv=5,  # 5-fold cross-validation\n",
    "                           verbose=1)  # Print progress\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model with the best parameters on the test set\n",
    "best_classifier = grid_search.best_estimator_\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Accuracy: {accuracy: .2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EmeNEWm_zXpK",
   "metadata": {
    "id": "EmeNEWm_zXpK"
   },
   "source": [
    "The grid search did not improve accuracy, but did improve the f1 score, which is what we were trying to optimize.  However, the overall results are not that significant.  Therefore, we conclude that we have got about the best model we can using these techniques.\n",
    "\n",
    "In conclusion, XGBoost did not improve over Random Forest, even with a grid search.  Feature engineering was necessary to boost performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
