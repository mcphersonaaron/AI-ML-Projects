{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd841ad5",
   "metadata": {
    "id": "bd841ad5"
   },
   "source": [
    "# Customer Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b515cb57",
   "metadata": {
    "id": "b515cb57"
   },
   "source": [
    "## Introduction to Customer Segmentation\n",
    "\n",
    "Customer segmentation is a powerful technique in data science that enables businesses to categorize their customers into distinct groups based on shared characteristics. This approach is pivotal in understanding customer behavior, optimizing marketing strategies, and enhancing customer service. In this assignment, we will delve into the practical application of customer segmentation using machine learning algorithms.\n",
    "\n",
    "- **Significance of Customer Segmentation**:\n",
    "  - **Targeted Marketing**: Tailoring marketing campaigns to specific customer groups based on their purchasing behavior and preferences.\n",
    "  - **Product Customization**: Developing products and services that cater to the specific needs and desires of different customer segments.\n",
    "  - **Improved Customer Experience**: Delivering personalized experiences to customers, increasing satisfaction and loyalty.\n",
    "\n",
    "The example code provided serves as a starting point for this exploration. It demonstrates the application of K-Means clustering, a popular technique in machine learning for grouping data. This algorithm partitions customers into clusters based on features like transaction amount, account balance, and transaction frequency.\n",
    "\n",
    "- **Key Techniques and Concepts**:\n",
    "  - **K-Means Clustering**: Understand and apply K-Means to segment customers.\n",
    "  - **Data Standardization**: Learn the importance of scaling features for effective clustering.\n",
    "  - **Cluster Visualization**: Gain skills in visualizing the clusters to extract meaningful insights.\n",
    "\n",
    "We will expand upon this initial code by experimenting with different numbers of clusters, applying additional clustering techniques like Hierarchical Clustering and DBSCAN, and performing a thorough analysis of the clusters to understand their business implications. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ae0ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "a38ae0ae",
    "outputId": "50f65ab9-43c0-4768-fb97-c6169854b93f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'TransactionAmount': np.random.uniform(10, 1000, 100),\n",
    "    'AccountBalance': np.random.uniform(500, 5000, 100),\n",
    "    'TransactionFrequency': np.random.poisson(5, 100)\n",
    "})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Choose the number of clusters (you may want to experiment with this)\n",
    "num_clusters = 3\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Visualize the clusters (for two features)\n",
    "plt.scatter(data['TransactionAmount'], data['AccountBalance'], c=data['Cluster'], cmap='viridis')\n",
    "plt.title('Customer Segmentation')\n",
    "plt.xlabel('Transaction Amount')\n",
    "plt.ylabel('Account Balance')\n",
    "plt.show()\n",
    "\n",
    "# Display the cluster centers (in the standardized feature space)\n",
    "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "cluster_centers_df = pd.DataFrame(cluster_centers, columns=data.columns[:-1])\n",
    "print(cluster_centers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ea90f",
   "metadata": {
    "id": "998ea90f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854f0fa",
   "metadata": {
    "id": "4854f0fa"
   },
   "outputs": [],
   "source": [
    "# Add additional diagrams\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Mall_Customers.csv'  # Replace with the actual file path\n",
    "data2 = pd.read_csv(file_path)\n",
    "\n",
    "# Select relevant features for clustering\n",
    "selected_features = data2[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data2 = scaler.fit_transform(selected_features)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "num_clusters = 5  # This can be adjusted based on experimentation\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "data2['Cluster'] = kmeans.fit_predict(scaled_data2)\n",
    "\n",
    "# Prepare for Hierarchical Clustering\n",
    "Z = linkage(scaled_data2, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279fb08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "e279fb08",
    "outputId": "3bc73a2f-e635-4d3b-c5e6-54e6ea482eca"
   },
   "outputs": [],
   "source": [
    "# Visualize the clusters (for two features)\n",
    "plt.scatter(data2['Annual Income (k$)'], data2['Spending Score (1-100)'], c=data2['Cluster'], cmap='viridis')\n",
    "plt.title('Customer Segmentation')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.show()\n",
    "\n",
    "# Display the cluster centers (in the original feature space)\n",
    "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "cluster_centers_df = pd.DataFrame(cluster_centers, columns=selected_features.columns)\n",
    "print(cluster_centers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cjfnjsLhMoDs",
   "metadata": {
    "id": "cjfnjsLhMoDs"
   },
   "source": [
    "The k-means algorithm has done a good job of identifying the clusters.  The centroids are well dispersed.  I excluded age from the analysis, because it caused there to be an extra cluster in the center, overlapping cluster 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E4S25L43Gf_G",
   "metadata": {
    "id": "E4S25L43Gf_G"
   },
   "source": [
    "**Data Exploration and Analysis**\n",
    "\n",
    "First, we look at the first five records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327f413",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7327f413",
    "outputId": "5efff872-f718-4a54-f65c-7e715b004149"
   },
   "outputs": [],
   "source": [
    "print(data2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VgAR-8F6HG4a",
   "metadata": {
    "id": "VgAR-8F6HG4a"
   },
   "source": [
    "Next, we examine the distribution of the variables. Annual Income is skewed right, but spending score is roughly normally distributed.  The scales actually by coincidence are pretty similar, but it makes sense to scale the data anyway for a K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7a22e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "45f7a22e",
    "outputId": "99dee281-395c-4db5-b123-26936f25008b"
   },
   "outputs": [],
   "source": [
    "# Generate histograms for the features\n",
    "data2[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].hist(figsize=(10, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a9fe45",
   "metadata": {
    "id": "05a9fe45"
   },
   "source": [
    "### Introduction to Cluster Evaluation Techniques\n",
    "\n",
    "In the realm of unsupervised machine learning, determining the optimal number of clusters is a pivotal decision that can significantly impact the outcomes of your model. Cluster evaluation techniques are essential tools that provide guidance in this decision-making process. Two of the most widely recognized methods for evaluating clustering results are the Elbow Method and the Silhouette Score.\n",
    "\n",
    "#### Elbow Method\n",
    "- **Explanation**: The Elbow Method is a heuristic used in determining the number of clusters in a data set. The approach involves plotting the explained variance as a function of the number of clusters, and picking the point where the increase in variance explained by adding another cluster is not significant anymore. This point is known as the 'elbow', where the graph bends.\n",
    "- **Interpretation**: In the Elbow Method, one should look for a change in the gradient of the line plot; a sharp change like an elbow suggests the optimal number of clusters. The idea is that adding more clusters beyond this number does not provide much better modeling of the data.\n",
    "\n",
    "#### Silhouette Score\n",
    "- **Explanation**: The Silhouette Score is a metric used to calculate the goodness of a clustering technique. It measures the distance between points within a cluster and the distance to points in the next nearest cluster. The score ranges from -1 to +1, where a high value indicates that the points are well clustered.\n",
    "- **Interpretation**: A Silhouette Score close to +1 indicates that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "Both methods provide different lenses through which to view the clustering results and can be used in conjunction to make a more informed decision. The Elbow Method gives us an insight into the variance within each cluster, whereas the Silhouette Score provides a measure of how similar an object is to its own cluster compared to others. The optimal number of clusters is often the one that balances between the two measures, subject to the specific context and use case of the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de011e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "27de011e",
    "outputId": "f5267767-f834-4f68-a761-afe0952345b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(scaled_data2)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WFfu4aUKIPIy",
   "metadata": {
    "id": "WFfu4aUKIPIy"
   },
   "source": [
    "The elbow chart shows that the optimum number of clusters is 6, as this is where the sum of squared distances between each point and the centroid of the cluster it belongs to starts to reach diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c80165",
   "metadata": {
    "id": "99c80165"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914aaa5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "5914aaa5",
    "outputId": "aef7b48c-8d3c-4ce9-9aee-816f0d3f1592"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "for i in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(scaled_data2)\n",
    "    score = silhouette_score(scaled_data2, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores)\n",
    "plt.title('Silhouette Score for each number of clusters')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6wRtYH_JAfb",
   "metadata": {
    "id": "a6wRtYH_JAfb"
   },
   "source": [
    "The Silhouette Score likewise points to an optimal number of clusters as 5, so our initial assumption about the number of clusters was correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24OLgKAyQUsI",
   "metadata": {
    "id": "24OLgKAyQUsI"
   },
   "source": [
    "**Advanced Implementation of Hierarchical Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28b031",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "ce28b031",
    "outputId": "d8c43fcc-a208-4c04-f6d9-1d2f8ee7b131"
   },
   "outputs": [],
   "source": [
    "# Plot the dendrogram with limited branches\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=10,  # number of clusters to show\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram (Truncated)')\n",
    "plt.xlabel('Cluster size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwWoRMh8Q0-J",
   "metadata": {
    "id": "iwWoRMh8Q0-J"
   },
   "source": [
    "Looking at the dendrogram, we can get most of the datapoints into clusters by drawing a line at distance 6. This identifies 5 clusters, which we saw before was the optimal number. I limited the number of clusters to 10 to avoid overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asHDutOISOTg",
   "metadata": {
    "id": "asHDutOISOTg"
   },
   "source": [
    "**Comprehensive Cluster Analysis**\n",
    "\n",
    "To go beyond the silhouette score, Gemini recommended two additional metrics as follows:\n",
    "\n",
    "1.   Calinski-Harabasz Index (Variance Ratio Criterion):\n",
    "\n",
    "\n",
    "*   Measures the ratio of between-cluster dispersion to within-cluster dispersion.\n",
    "*   A higher Calinski-Harabasz score generally indicates better-defined clusters.\n",
    "*   It's calculated as the sum of squared distances between clusters divided by the sum of squared distances within clusters.\n",
    "\n",
    "2.   Davies-Bouldin Index:\n",
    "\n",
    "*  Measures the average similarity ratio of each cluster with the cluster that is most similar to it.\n",
    "*  Similarity is a measure based on distances between centroids and the spread of points within clusters.\n",
    "*  A lower Davies-Bouldin index indicates better clustering, with clusters that are more separated and less spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MyJZMROeTesW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyJZMROeTesW",
    "outputId": "53465af0-2a09-44bc-cd31-71aa86722841"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "calinski_score = calinski_harabasz_score(scaled_data2, kmeans.labels_)\n",
    "davies_bouldin = davies_bouldin_score(scaled_data2, kmeans.labels_)\n",
    "\n",
    "print(f\"Calinski-Harabasz Index: {calinski_score}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZWyDFJZMTs93",
   "metadata": {
    "id": "ZWyDFJZMTs93"
   },
   "source": [
    "As one would want, the Calinski-Harabasz Index is relatively high, and the Davies-Bouldin Index is relatively low, indicating that the clusters are widely dispersed and that the datapoints within each cluster are fairly similar to each other.\n",
    "\n",
    "**Final Analysis**\n",
    "\n",
    "The cluster analysis reveals that dataspace is divided into four quadrants, each containing a different combination of high and low annual income and high and low spending.\n",
    "\n",
    "*  For example, cluster 1 has high spending and high income. You might call these your *whales* - they are the customers that have both the ability and willingness to spend.  \n",
    "*  Cluster 2, on the other hand, has low income but high spend.  You might call these the *fans*, because they spend a higher percentage of their income on your product.  \n",
    "*  Cluster 3 has high income but low spending: these are your *targets,* because they have the capacity to be spending more than they currently are.  \n",
    "*  Cluster 4 has both low income and low spend, so they are *strugglers.*  \n",
    "*  Finally, cluster 0 is all those who are in the middle, having moderate income and moderate spending.  They might be *secondary targets*, because their spending ability is limited, but they could be doing more than they are.\n",
    "\n",
    "Through this analysis, we can identify the customers we should be pitching offers to in an effort to get them to spend more with us, and avoid wasting time and money on customers that have little ability to spend more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324368d",
   "metadata": {
    "id": "3324368d"
   },
   "source": [
    "- **DBSCAN Clustering**:\n",
    "  - Implement DBSCAN and compare its segmentation with K-Means and Hierarchical clustering.\n",
    "  - Analyze the clusters formed by DBSCAN for any unique characteristics.\n",
    "\n",
    "- **Principal Component Analysis (PCA)**:\n",
    "  - Apply PCA to the data and visualize the results.\n",
    "  - Discuss how dimensionality reduction impacts the clustering results and its potential use in simplifying complex datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e0144",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "e39e0144",
    "outputId": "43412b41-1376-4b5a-a44c-c8ca4683b7e9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "epsilon = 0.5  # I tried different values until I got a low number of noise points.\n",
    "min_samples = 10 # Tried different values, this seemed like a good balance.\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "data2['DBSCAN_Cluster'] = dbscan.fit_predict(scaled_data2)\n",
    "\n",
    "# The cluster labels in DBSCAN include -1 for noise points\n",
    "# You can explore the number of clusters found and the number of noise points\n",
    "n_clusters_ = len(set(data2['DBSCAN_Cluster'])) - (1 if -1 in data2['DBSCAN_Cluster'] else 0)\n",
    "n_noise_ = list(data2['DBSCAN_Cluster']).count(-1)\n",
    "\n",
    "print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "print(f\"Estimated number of noise points: {n_noise_}\")\n",
    "\n",
    "# Visualize the DBSCAN clusters\n",
    "plt.scatter(data2['Annual Income (k$)'], data2['Spending Score (1-100)'], c=data2['DBSCAN_Cluster'], cmap='viridis')\n",
    "plt.title('DBSCAN Customer Segmentation')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RSnR6Licbhkt",
   "metadata": {
    "id": "RSnR6Licbhkt"
   },
   "source": [
    "The clustering with DBSCAN is similar to that with k-means, although DBSCAN classifies as noise some outlying points that k-means identifies as being in the clusters.  Adjusting epsilon up or down tends to reduce the number of clusters, and I like the five that we established earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009656dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "009656dd",
    "outputId": "ac573f1f-2e84-4021-8365-ed7209cdfbab"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce the data to 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_data2)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "# Add the cluster labels to the PCA DataFrame (using the KMeans clusters for visualization)\n",
    "pca_df['Cluster'] = data2['Cluster'] # You can use DBSCAN clusters here as well\n",
    "\n",
    "# Visualize the data in the PCA space\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], c=pca_df['Cluster'], cmap='viridis')\n",
    "plt.title('Customer Segmentation (PCA Reduced)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# You can also examine the explained variance ratio of each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance ratio of principal components: {explained_variance_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KN3vebhxeE8z",
   "metadata": {
    "id": "KN3vebhxeE8z"
   },
   "source": [
    "Although we only had two features to begin with, using PCA shows that they are about equally important.\n",
    "\n",
    "The analysis remains the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdJjqsK-eQ37",
   "metadata": {
    "id": "qdJjqsK-eQ37"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (aiml)",
   "language": "python",
   "name": "aiml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
